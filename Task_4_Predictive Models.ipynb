{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbf220f",
   "metadata": {},
   "source": [
    "## Kaggle Boston Housing Dataset\n",
    "## Source: https://www.kaggle.com/datasets/abhijithudayakumar/the-boston-housing-dataset\n",
    "## Importing necessary libraries\n",
    "\n",
    "- `import pandas as pd`  \n",
    "  Imports the **pandas** library, which is commonly used for handling structured/tabular data.\n",
    "\n",
    "- `path = ...`  \n",
    "  Stores the file path of the **Boston Housing CSV** dataset as a string.\n",
    "\n",
    "- `pd.read_csv(path)`  \n",
    "  Reads the CSV file located at the specified path into a **DataFrame** named `df`.\n",
    "\n",
    "- `df.shape`  \n",
    "  Returns a tuple indicating the number of **rows and columns** in the dataset, e.g., `(506, 14)`.\n",
    "\n",
    "- `df.columns.tolist()`  \n",
    "  Lists the **names of all columns (features)** in the dataset as a Python list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec2cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14) ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n"
     ]
    }
   ],
   "source": [
    "# Import the pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the dataset (CSV file) on your local machine\n",
    "path = (r\"C:\\Users\\Ak\\Desktop\\Developers Hub Internship\\Task 4 Predicting House Prices Using the Boston Housing Dataset\\boston.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame using pandas\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Print the shape (rows, columns) and the list of column names\n",
    "print(df.shape, df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543fc5c",
   "metadata": {},
   "source": [
    "###  Feature Scaling Explanation\n",
    "\n",
    "- `from sklearn.preprocessing import StandardScaler`  \n",
    "  Imports the `StandardScaler` from Scikit-learn, which standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "- `df.select_dtypes(...)`  \n",
    "  Selects all numeric columns (`int64` and `float64`) from the dataset.\n",
    "\n",
    "- `.drop('MEDV')`  \n",
    "  Removes the target variable `'MEDV'` from the list of columns to be scaled.\n",
    "\n",
    "- `scaler = StandardScaler()`  \n",
    "  Creates an instance of the standard scaler.\n",
    "\n",
    "- `scaler.fit_transform(df[num_cols])`  \n",
    "  Fits the scaler on the numeric data and transforms it, replacing the original values with standardized ones (mean = 0, standard deviation = 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb6d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # Import the scaler to normalize numerical features\n",
    "\n",
    "# Select all numeric columns except the target variable 'MEDV'\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop('MEDV')\n",
    "\n",
    "scaler = StandardScaler()                         # Create a StandardScaler instance\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols]) # Apply standardization (mean=0, std=1) to numeric columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dded27",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "- `from sklearn.model_selection import train_test_split`  \n",
    "  Imports the function to split the dataset into training and testing sets.\n",
    "\n",
    "- `X = df.drop(columns='MEDV')`  \n",
    "  Selects all columns **except** `'MEDV'` as feature variables (`X`).\n",
    "\n",
    "- `y = df['MEDV']`  \n",
    "  Selects `'MEDV'` as the target variable (`y`), which represents the house prices.\n",
    "\n",
    "- `train_test_split(...)`  \n",
    "  Splits the dataset into training and testing subsets:\n",
    "  - `test_size=0.2` means 20% of the data is used for testing.\n",
    "  - `random_state=42` ensures reproducibility of the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0410396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # Import function to split the dataset\n",
    "\n",
    "X = df.drop(columns='MEDV')   # Features: all columns except 'MEDV'\n",
    "y = df['MEDV']                # Target: the column to predict\n",
    "\n",
    "# Split data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a9ca5",
   "metadata": {},
   "source": [
    "### Converting 'CHAS' to Integer\n",
    "\n",
    "- `df['CHAS'] = df['CHAS'].astype(int)`  \n",
    "  The `'CHAS'` column contains binary values (`0` or `1`) as strings.  \n",
    "  This line converts them into integer type for proper numerical analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d461ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CHAS'] = df['CHAS'].astype(int)   # Convert 'CHAS' column to integer type (from string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02db46b",
   "metadata": {},
   "source": [
    "###  LinearRegressionScratch (from scratch)\n",
    "\n",
    "This class implements **Linear Regression** using the **Normal Equation**:\n",
    "\n",
    "1. `fit(X, y)`:\n",
    "   - Adds a bias column to the features.\n",
    "   - Calculates the best-fit parameters using matrix algebra:\n",
    "     \\[\n",
    "     \\theta = (X^TX)^{-1}X^Ty\n",
    "     \\]\n",
    "   - Stores the intercept and coefficients.\n",
    "\n",
    "2. `predict(X)`:\n",
    "   - Uses the learned parameters to predict target values.\n",
    "\n",
    "Note: This is a simplified version and assumes no multicollinearity (X^T X must be invertible).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ebfa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    def __init__(self):\n",
    "        self.coef_ = None         # Stores learned coefficients (slopes)\n",
    "        self.intercept_ = None    # Stores the y-intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Add a column of ones to X to account for the intercept\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Closed-form solution (Normal Equation)\n",
    "        theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "        # Extract the intercept and coefficients\n",
    "        self.intercept_ = theta_best[0]\n",
    "        self.coef_ = theta_best[1:]\n",
    "        \n",
    "        return self  # Allows method chaining\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Returns predicted values based on learned parameters\n",
    "        return X.dot(self.coef_) + self.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbc53f",
   "metadata": {},
   "source": [
    "###  DecisionTreeScratch (from scratch)\n",
    "\n",
    "This is a manual implementation of a **Decision Tree Regressor** for numerical target prediction.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- **fit(X, y)**: Builds the tree recursively by selecting the best feature and threshold that minimize **Mean Squared Error (MSE)**.\n",
    "- **predict(X)**: Uses the trained tree to predict values for new samples.\n",
    "- **Feature Sampling**: `max_features` allows random subsetting of features (used in Random Forest).\n",
    "- **Stopping Criteria**:\n",
    "  - Minimum number of samples (`min_samples_split`)\n",
    "  - Maximum tree depth (`max_depth`)\n",
    "  - No further improvement in splitting\n",
    "\n",
    "#### Notes:\n",
    "- It does not support categorical features.\n",
    "- It does not perform pruning.\n",
    "- Designed for educational purposes and comparison with library models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4159852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeScratch:\n",
    "    def __init__(self, max_features=None, max_depth=None, min_samples_split=2):\n",
    "        self.max_features = max_features              # Number of features to consider at each split\n",
    "        self.max_depth = max_depth                    # Maximum depth of the tree\n",
    "        self.min_samples_split = min_samples_split    # Minimum number of samples to split\n",
    "        self.feature_index = None                     # Index of the feature used for the split\n",
    "        self.threshold = None                         # Threshold value for the split\n",
    "        self.left_child = None                        # Left subtree\n",
    "        self.right_child = None                       # Right subtree\n",
    "        self.value = None                             # Value for leaf nodes (mean prediction)\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Stopping condition: leaf node\n",
    "        if n_samples < self.min_samples_split or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            self.value = np.mean(y)\n",
    "            return\n",
    "\n",
    "        # Feature sampling (used in Random Forests)\n",
    "        features = (\n",
    "            np.random.choice(n_features, self.max_features, replace=False)\n",
    "            if self.max_features else range(n_features)\n",
    "        )\n",
    "\n",
    "        # Find best split by minimizing MSE\n",
    "        best_mse = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in features:\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Compute MSE for left and right splits\n",
    "                left_mean = np.mean(y[left_mask])\n",
    "                right_mean = np.mean(y[right_mask])\n",
    "                mse_left = np.mean((y[left_mask] - left_mean) ** 2)\n",
    "                mse_right = np.mean((y[right_mask] - right_mean) ** 2)\n",
    "                mse = (mse_left * np.sum(left_mask) + mse_right * np.sum(right_mask)) / n_samples\n",
    "\n",
    "                # Keep the best split\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if best_feature is None:\n",
    "            self.value = np.mean(y)\n",
    "            return\n",
    "\n",
    "        # Store split details\n",
    "        self.feature_index = best_feature\n",
    "        self.threshold = best_threshold\n",
    "\n",
    "        # Recursively grow left and right branches\n",
    "        left_indices = X[:, self.feature_index] < self.threshold\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        self.left_child = DecisionTreeScratch(max_features=self.max_features, max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "        self.left_child.fit(X[left_indices], y[left_indices], depth + 1)\n",
    "\n",
    "        self.right_child = DecisionTreeScratch(max_features=self.max_features, max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "        self.right_child.fit(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        # Predict a single instance by traversing the tree\n",
    "        if self.value is not None:\n",
    "            return self.value\n",
    "\n",
    "        if x[self.feature_index] < self.threshold:\n",
    "            return self.left_child.predict_single(x)\n",
    "        else:\n",
    "            return self.right_child.predict_single(x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict multiple samples\n",
    "        return np.array([self.predict_single(x) for x in X])\n",
    "\n",
    "    def get_split_feature_index(self):\n",
    "        # Used for feature importance (e.g., in Random Forest)\n",
    "        return self.feature_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c1728",
   "metadata": {},
   "source": [
    "# RandomForestScratch Class\n",
    "\n",
    "This class implements a simplified version of a Random Forest model from scratch. It builds an ensemble of decision trees and averages their outputs for regression tasks.\n",
    "\n",
    "## Parameters:\n",
    "- `n_estimators`: Number of decision trees in the forest (default: 10).\n",
    "- `max_features`: Maximum number of features considered when splitting a node.\n",
    "- `max_depth`: Maximum depth each tree can grow.\n",
    "- `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "\n",
    "## Methods:\n",
    "\n",
    "### `fit(X, y)`\n",
    "Trains the random forest model.\n",
    "- Uses bootstrap aggregation (bagging) to sample training data for each tree.\n",
    "- Trains each `DecisionTreeScratch` on a different sample.\n",
    "\n",
    "### `predict(X)`\n",
    "Predicts the output by averaging the predictions from all trees.\n",
    "\n",
    "### `get_feature_importance_array(n_features=None)`\n",
    "Returns an array of feature importances.\n",
    "- Counts how often each feature was used to split across all trees.\n",
    "- Normalizes the result so that the importances sum to 1 (or the average contribution per tree).\n",
    "\n",
    "> **Note**: This implementation assumes the existence of a `DecisionTreeScratch` class with a `.predict()` method and a `.get_split_feature_index()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1456e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestScratch:\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_depth=None, min_samples_split=2):\n",
    "        # Initialize the random forest with parameters for number of trees, features per split, depth, and minimum samples\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []  # List to hold individual decision trees\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fit the forest to the data\n",
    "        self.n_features_ = X.shape[1]  # Save number of features\n",
    "        self.trees = []  # Clear any existing trees\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sampling: randomly sample data points with replacement\n",
    "            idx = np.random.choice(len(X), len(X), replace=True)\n",
    "            X_sample, y_sample = X[idx], y[idx]\n",
    "\n",
    "            # Create and train a new decision tree on the sampled data\n",
    "            tree = DecisionTreeScratch(\n",
    "                max_features=self.max_features,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "\n",
    "            # Add the trained tree to the forest\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Aggregate predictions from all trees\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Return average prediction across trees (suitable for regression)\n",
    "        return np.mean(tree_preds, axis=0)\n",
    "\n",
    "    def get_feature_importance_array(self, n_features=None):\n",
    "        # Compute how frequently each feature was used to split nodes\n",
    "        if n_features is None:\n",
    "            n_features = self.n_features_\n",
    "        \n",
    "        importances = np.zeros(n_features)\n",
    "        for tree in self.trees:\n",
    "            idx = tree.get_split_feature_index()\n",
    "            if idx is not None:\n",
    "                importances[idx] += 1\n",
    "        \n",
    "        # Normalize by number of trees to get relative importance\n",
    "        return importances / len(self.trees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ff78f",
   "metadata": {},
   "source": [
    "# OGBoostScratch CLass\n",
    "\n",
    "This class implements a basic version of Gradient Boosting for regression, inspired by popular libraries like XGBoost and LightGBM.\n",
    "\n",
    "## Parameters:\n",
    "- **n_estimators** (`int`): Number of boosting iterations (default: 100).\n",
    "- **learning_rate** (`float`): Shrinkage rate applied to tree outputs (default: 0.1).\n",
    "- **max_features** (`int` or `None`): Number of features to consider when looking for the best split.\n",
    "- **max_depth** (`int` or `None`): Maximum depth of each decision tree.\n",
    "- **min_samples_split** (`int`): Minimum samples required to split a node.\n",
    "- **early_stopping_rounds** (`int` or `None`): Number of rounds with no improvement on validation set to stop training early.\n",
    "- **validation_fraction** (`float`): Fraction of training data used for validation if early stopping is enabled.\n",
    "- **tol** (`float`): Minimum improvement in validation loss to reset early stopping counter.\n",
    "- **random_state** (`int` or `None`): Seed for reproducibility.\n",
    "\n",
    "## Methods:\n",
    "\n",
    "### `fit(X, y)`\n",
    "Trains the boosting model on dataset `(X, y)`:\n",
    "- Initializes the model with the mean of `y`.\n",
    "- Iteratively fits trees on residuals (errors) and updates predictions.\n",
    "- Supports optional early stopping using a validation split.\n",
    "\n",
    "### `predict(X)`\n",
    "Predicts output values for the input data `X` using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ba745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGBoostScratch:\n",
    "    def __init__(self, \n",
    "                 n_estimators=100, \n",
    "                 learning_rate=0.1, \n",
    "                 max_features=None,\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 early_stopping_rounds=None,\n",
    "                 validation_fraction=0.1,\n",
    "                 tol=1e-4,\n",
    "                 random_state=None):\n",
    "        # Initialize hyperparameters for gradient boosting\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.trees = []      # List to store individual trees\n",
    "        self.init_pred = None  # Initial prediction (usually mean of y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Set seed for reproducibility\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # If early stopping is enabled, split into training and validation sets\n",
    "        if self.early_stopping_rounds is not None:\n",
    "            val_size = int(n_samples * self.validation_fraction)\n",
    "            idxs = np.arange(n_samples)\n",
    "            np.random.shuffle(idxs)\n",
    "            train_idx, val_idx = idxs[val_size:], idxs[:val_size]\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_val, y_val = X[val_idx], y[val_idx]\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "            X_val, y_val = None, None\n",
    "\n",
    "        # Initial prediction: mean of training labels\n",
    "        self.init_pred = np.mean(y_train)\n",
    "        F = np.full_like(y_train, fill_value=self.init_pred, dtype=float)  # Predicted values\n",
    "\n",
    "        self.trees = []\n",
    "        best_val_loss = float('inf')\n",
    "        rounds_no_improve = 0\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute residuals (negative gradients for MSE loss)\n",
    "            residuals = y_train - F\n",
    "\n",
    "            # Fit a regression tree on the residuals\n",
    "            tree = DecisionTreeScratch(\n",
    "                max_features=self.max_features,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            tree.fit(X_train, residuals)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Update prediction with scaled tree output\n",
    "            update = self.learning_rate * tree.predict(X_train)\n",
    "            F += update\n",
    "\n",
    "            # Early stopping check\n",
    "            if X_val is not None:\n",
    "                val_pred = self.predict(X_val)\n",
    "                val_loss = np.mean((y_val - val_pred) ** 2)\n",
    "                if best_val_loss - val_loss > self.tol:\n",
    "                    best_val_loss = val_loss\n",
    "                    rounds_no_improve = 0\n",
    "                else:\n",
    "                    rounds_no_improve += 1\n",
    "                    if rounds_no_improve >= self.early_stopping_rounds:\n",
    "                        print(f\"Early stopping at iteration {i+1}, best val loss: {best_val_loss:.5f}\")\n",
    "                        break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict using the ensemble: initial prediction + sum of tree outputs\n",
    "        y_pred = np.full((X.shape[0],), fill_value=self.init_pred, dtype=float)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def get_feature_importance_array(self, n_features=None):\n",
    "        # Aggregate feature importances from all trees\n",
    "        if n_features is None:\n",
    "            try:\n",
    "                n_features = self.trees[0].n_features_\n",
    "            except IndexError:\n",
    "                raise ValueError(\"Please pass n_features or fit the model first.\")\n",
    "\n",
    "        importances = np.zeros(n_features, dtype=float)\n",
    "        for tree in self.trees:\n",
    "            importances += tree.get_feature_importance_array(n_features)\n",
    "        importances /= len(self.trees)  # Normalize importances\n",
    "        return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110bf94",
   "metadata": {},
   "source": [
    "# CrossValScoreScratch\n",
    "\n",
    "This function performs k-fold cross-validation for models built from scratch.\n",
    "\n",
    "## Parameters:\n",
    "- **model_class** (`class`): The custom model class to be used (e.g., `RandomForestScratch`, `OGBoostScratch`).\n",
    "- **X** (`numpy.ndarray`): Feature matrix.\n",
    "- **y** (`numpy.ndarray`): Target vector.\n",
    "- **cv** (`int`, default = 5): Number of folds in cross-validation.\n",
    "- **model_params**: Additional keyword arguments to initialize the model.\n",
    "\n",
    "## Returns:\n",
    "- **mean_rmse** (`float`): Average Root Mean Squared Error across all folds.\n",
    "- **mean_r2** (`float`): Average R² (coefficient of determination) score across all folds.\n",
    "\n",
    "## How it Works:\n",
    "1. Randomly shuffles the dataset.\n",
    "2. Splits it into `cv` folds.\n",
    "3. Iteratively trains the model on `cv-1` folds and evaluates it on the remaining fold.\n",
    "4. Collects RMSE and R² metrics for each fold.\n",
    "5. Returns the average RMSE and R².\n",
    "\n",
    "> This function is useful for comparing model performance using standard cross-validation metrics, especially when working with custom machine learning models built from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation function for scratch-built models\n",
    "def cross_val_score_scratch(model_class, X, y, cv=5, **model_params):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)  # Array of all sample indices\n",
    "    np.random.shuffle(indices)      # Shuffle indices for randomness\n",
    "\n",
    "    # Determine fold sizes for cross-validation\n",
    "    fold_sizes = (n_samples // cv) * np.ones(cv, dtype=int)\n",
    "    fold_sizes[:n_samples % cv] += 1  # Distribute remaining samples to first few folds\n",
    "\n",
    "    current = 0\n",
    "    rmses = []  # Store RMSE scores for each fold\n",
    "    r2s = []    # Store R² scores for each fold\n",
    "\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        val_idx = indices[start:stop]  # Indices for validation fold\n",
    "        train_idx = np.concatenate([indices[:start], indices[stop:]])  # Rest is training data\n",
    "\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "        # Initialize and train model with given parameters\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on validation set\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        # Compute RMSE\n",
    "        rmse = np.sqrt(np.mean((y_val - preds) ** 2))\n",
    "\n",
    "        # Compute R² score\n",
    "        r2 = 1 - np.sum((preds - y_val) ** 2) / np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "\n",
    "        # Store metrics\n",
    "        rmses.append(rmse)\n",
    "        r2s.append(r2)\n",
    "\n",
    "        current = stop\n",
    "\n",
    "    # Return average RMSE and R² across folds\n",
    "    return np.mean(rmses), np.mean(r2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd75c30",
   "metadata": {},
   "source": [
    "# Model Comparison: Linear Regression, Random Forest, and Gradient Boosting (Scratch)\n",
    "\n",
    "This script evaluates the performance of three machine learning models implemented from scratch.\n",
    "\n",
    "## 1. Metric Functions\n",
    "- `rmse(y_true, y_pred)`: Computes Root Mean Squared Error (lower is better).\n",
    "- `r2_score(y_true, y_pred)`: Computes the R² score (closer to 1 is better).\n",
    "\n",
    "## 2. Model Training\n",
    "- **LinearRegressionScratch**: Trained on `X_train`, `y_train`.\n",
    "- **RandomForestScratch**: Trained with 50 trees and a maximum of 5 features per split.\n",
    "- **OGBoostScratch**: Gradient boosting with 50 iterations and a learning rate of 0.05.\n",
    "\n",
    "## 3. Prediction and Evaluation\n",
    "Each model predicts on `X_test`, and the predictions are evaluated using RMSE and R² metrics. The results are printed in a loop for easy comparison.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b45507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionScratch → RMSE =  4.929 | R² =  0.669\n",
      "RandomForestScratch  → RMSE =  2.824 | R² =  0.891\n",
      "OGBoostScratch       → RMSE =  2.792 | R² =  0.894\n"
     ]
    }
   ],
   "source": [
    "# Metric functions \n",
    "def rmse(y_true, y_pred):\n",
    "    # Computes Root Mean Squared Error\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    # Computes R-squared (coefficient of determination)\n",
    "    return 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "\n",
    "# 1. Train all models \n",
    "\n",
    "# Linear Regression (from scratch)\n",
    "lin = LinearRegressionScratch().fit(X_train.values, y_train.values)\n",
    "\n",
    "# Random Forest with 50 trees and 5 features max per split\n",
    "rf = RandomForestScratch(\n",
    "    n_estimators=50,\n",
    "    max_features=5\n",
    ").fit(X_train.values, y_train.values)\n",
    "\n",
    "# Gradient Boosting without setting max_depth\n",
    "ogb = OGBoostScratch(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.05,\n",
    "    max_features=5\n",
    ").fit(X_train.values, y_train.values)\n",
    "\n",
    "# 2. Predict on test set \n",
    "y_pred_lin = lin.predict(X_test.values)\n",
    "y_pred_rf  = rf.predict(X_test.values)\n",
    "y_pred_ogb = ogb.predict(X_test.values)\n",
    "\n",
    "#  3. Evaluate and print metrics for each model \n",
    "for name, y_pred in [\n",
    "    (\"LinearRegressionScratch\", y_pred_lin),\n",
    "    (\"RandomForestScratch\",     y_pred_rf),\n",
    "    (\"OGBoostScratch\",          y_pred_ogb),\n",
    "]:\n",
    "    e_rmse = rmse(y_test.values, y_pred)\n",
    "    e_r2   = r2_score(y_test.values, y_pred)\n",
    "    print(f\"{name:20s} → RMSE = {e_rmse: .3f} | R² = {e_r2: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b10de",
   "metadata": {},
   "source": [
    "# Model Performance Comparison: Scratch vs Library\n",
    "\n",
    "This script compares the performance of scratch-implemented machine learning models with their library-based counterparts using RMSE and R² metrics.\n",
    "\n",
    "## Models Compared:\n",
    "- **Scratch Models**:\n",
    "  - LinearScratch (custom Linear Regression)\n",
    "  - RandomForestScratch (custom Random Forest)\n",
    "  - OGBoostScratch (custom Gradient Boosting)\n",
    "\n",
    "- **Library Models**:\n",
    "  - LinearRegression (sklearn)\n",
    "  - RandomForestRegressor (sklearn)\n",
    "  - XGBRegressor (XGBoost)\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **RMSE**: Root Mean Squared Error (lower is better)\n",
    "- **R² Score**: Coefficient of determination (closer to 1 is better)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19052315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           RMSE        R2\n",
      "Model                                    \n",
      "LinearScratch          4.928602  0.668759\n",
      "RandomForestScratch    3.116742  0.867536\n",
      "OGBoostScratch         2.816042  0.891863\n",
      "LinearRegression       4.928602  0.668759\n",
      "RandomForestRegressor  3.198035  0.860536\n",
      "XGBRegressor           2.557355  0.910818\n"
     ]
    }
   ],
   "source": [
    "# 1. Import “library” models\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.ensemble        import RandomForestRegressor\n",
    "from xgboost                 import XGBRegressor   # or: from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# 2) Define your scratch models (assuming you already have these)\n",
    "# lin_scratch = LinearRegressionScratch().fit(...)\n",
    "# rf_scratch  = RandomForestScratch(...).fit(...)\n",
    "# ogb_scratch = OGBoostScratch(...).fit(...)\n",
    "\n",
    "# 3. Instantiate and fit the library versions\n",
    "models_lib = {\n",
    "    'LinearRegression'        : LinearRegression(),\n",
    "    'RandomForestRegressor'   : RandomForestRegressor(n_estimators=50, max_features=5, random_state=42),\n",
    "    'XGBRegressor'            : XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "for name, mdl in models_lib.items():\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "# 4. Collect predictions & metrics\n",
    "results = []\n",
    "\n",
    "# — scratch models\n",
    "for name, y_pred in [\n",
    "    ('LinearScratch',        lin.predict(X_test.values)),\n",
    "    ('RandomForestScratch',  rf.predict(X_test.values)),\n",
    "    ('OGBoostScratch',       ogb.predict(X_test.values)),\n",
    "]:\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE' : rmse(y_test.values, y_pred),\n",
    "        'R2'   : r2_score(y_test.values, y_pred)\n",
    "    })\n",
    "\n",
    "# library models\n",
    "for name, mdl in models_lib.items():\n",
    "    y_pred = mdl.predict(X_test)  # sklearn/XGB accept DataFrame or array\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE' : rmse(y_test.values, y_pred),\n",
    "        'R2'   : r2_score(y_test.values, y_pred)\n",
    "    })\n",
    "\n",
    "# 5. Display comparison\n",
    "import pandas as pd\n",
    "compare_df = pd.DataFrame(results).set_index('Model')\n",
    "print(compare_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab265417",
   "metadata": {},
   "source": [
    "#  Model Performance Comparison: Scratch vs Library Implementations\n",
    "\n",
    "This notebook compares custom-implemented machine learning models with their well-established library counterparts using two key regression metrics:\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**: Measures average prediction error magnitude. Lower is better.\n",
    "- **R² Score (Coefficient of Determination)**: Measures goodness-of-fit. Closer to 1 is better.\n",
    "\n",
    "---\n",
    "\n",
    "## Models Evaluated\n",
    "\n",
    "### 1. **Scratch Implementations**\n",
    "| Name                 | Description                         |\n",
    "|----------------------|-------------------------------------|\n",
    "| `LinearScratch`      | Linear regression from scratch      |\n",
    "| `RandomForestScratch`| Random forest ensemble (custom)     |\n",
    "| `OGBoostScratch`     | Gradient boosting from scratch      |\n",
    "\n",
    "### 2. **Library Implementations**\n",
    "| Model                 | Library   | Key Parameters                        |\n",
    "|------------------------|------------|----------------------------------------|\n",
    "| `LinearRegression`     | `sklearn`  | Default settings                       |\n",
    "| `RandomForestRegressor`| `sklearn`  | 50 trees, `max_features=5`, `seed=42` |\n",
    "| `XGBRegressor`         | `xgboost`  | 100 estimators, LR=0.1, `depth=3`     |\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Results\n",
    "\n",
    "Below is a comparison of each model's performance on the test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d615c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           RMSE        R2\n",
      "Model                                    \n",
      "LinearScratch          4.928602  0.668759\n",
      "RandomForestScratch    2.824284  0.891229\n",
      "OGBoostScratch         2.792059  0.893697\n",
      "LinearRegression       4.928602  0.668759\n",
      "RandomForestRegressor  3.198035  0.860536\n",
      "XGBRegressor           2.557355  0.910818\n"
     ]
    }
   ],
   "source": [
    "# 1. Import standard ML models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor  # Could also use GradientBoostingRegressor\n",
    "\n",
    "# 2. Define your scratch models (already trained earlier)\n",
    "# lin, rf, ogb are assumed to be fitted instances of your scratch models\n",
    "\n",
    "# 3. Instantiate and fit the equivalent sklearn/xgboost models\n",
    "models_lib = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(\n",
    "        n_estimators=50, max_features=5, random_state=42\n",
    "    ),\n",
    "    'XGBRegressor': XGBRegressor(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Fit each library model on the training data\n",
    "for name, mdl in models_lib.items():\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate all models and collect metrics\n",
    "results = []\n",
    "\n",
    "# — Add results from scratch implementations\n",
    "for name, y_pred in [\n",
    "    ('LinearScratch', lin.predict(X_test.values)),\n",
    "    ('RandomForestScratch', rf.predict(X_test.values)),\n",
    "    ('OGBoostScratch', ogb.predict(X_test.values)),\n",
    "]:\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse(y_test.values, y_pred),\n",
    "        'R2': r2_score(y_test.values, y_pred)\n",
    "    })\n",
    "\n",
    "# Add results from library implementations\n",
    "for name, mdl in models_lib.items():\n",
    "    y_pred = mdl.predict(X_test)\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse(y_test.values, y_pred),\n",
    "        'R2': r2_score(y_test.values, y_pred)\n",
    "    })\n",
    "\n",
    "# 5. Display comparison as a DataFrame\n",
    "import pandas as pd\n",
    "compare_df = pd.DataFrame(results).set_index('Model')\n",
    "print(compare_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
